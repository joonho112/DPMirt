<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Mathematical Foundations: IRT Models and Dirichlet Process Mixture Priors ‚Ä¢ DPMirt</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="‚Äùimage/svg+xml‚Äù" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Mathematical Foundations: IRT Models and Dirichlet Process Mixture Priors">
<meta name="description" content="A comprehensive treatment of the mathematical foundations underlying the DPMirt package, covering Item Response Theory models (Rasch, 2PL, 3PL), identification and post-hoc rescaling, Dirichlet Process Mixture priors, the concentration parameter, and posterior summary theory (PM, CB, GR).
">
<meta property="og:description" content="A comprehensive treatment of the mathematical foundations underlying the DPMirt package, covering Item Response Theory models (Rasch, 2PL, 3PL), identification and post-hoc rescaling, Dirichlet Process Mixture priors, the concentration parameter, and posterior summary theory (PM, CB, GR).
">
<meta property="og:image" content="https://joonho112.github.io/DPMirt/logo.svg">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">DPMirt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-vignettes" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Vignettes</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-vignettes">
<li><h6 class="dropdown-header" data-toc-skip>--- Getting Started ---</h6></li>
    <li><a class="dropdown-item" href="../articles/introduction.html">Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/quick-start.html">Quick Start</a></li>
    <li><a class="dropdown-item" href="../articles/models-and-workflow.html">Models &amp; Workflow</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>--- Theory &amp; Methods ---</h6></li>
    <li><a class="dropdown-item" href="../articles/theory-irt-dpm.html">IRT &amp; DPM Theory</a></li>
    <li><a class="dropdown-item" href="../articles/posterior-summaries.html">Posterior Summaries</a></li>
    <li><a class="dropdown-item" href="../articles/prior-elicitation.html">Prior Elicitation</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>--- Advanced ---</h6></li>
    <li><a class="dropdown-item" href="../articles/nimble-internals.html">NIMBLE Internals</a></li>
    <li><a class="dropdown-item" href="../articles/simulation-study.html">Simulation Study</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/joonho112/DPMirt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Mathematical Foundations: IRT Models and Dirichlet Process Mixture Priors</h1>
                        <h4 data-toc-skip class="author">JoonHo Lee</h4>
            
            <h4 data-toc-skip class="date">2026-02-11</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/joonho112/DPMirt/blob/HEAD/vignettes/theory-irt-dpm.Rmd" class="external-link"><code>vignettes/theory-irt-dpm.Rmd</code></a></small>
      <div class="d-none name"><code>theory-irt-dpm.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">1. Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>This vignette provides a rigorous mathematical treatment of the
theory underlying the DPMirt package. It is intended for statisticians
and methodologically-inclined researchers who wish to understand the
full probabilistic framework connecting Item Response Theory (IRT)
measurement models, Bayesian hierarchical priors, Dirichlet Process
Mixture (DPM) extensions, identification strategies, and posterior
summary methods.</p>
<p>We cover the following topics in depth:</p>
<ol style="list-style-type: decimal">
<li>The Bayesian Rasch model and its hierarchical structure</li>
<li>The 2PL and 3PL extensions</li>
<li>Identification indeterminacy and post-hoc rescaling</li>
<li>Dirichlet Process Mixture priors for the latent trait
distribution</li>
<li>The concentration parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
and its hyperprior</li>
<li>Posterior summary theory: PM, CB, and GR estimators</li>
<li>DP density reconstruction from MCMC output</li>
</ol>
<p>Throughout, we carefully distinguish between <strong>established
results</strong> from the IRT, Bayesian nonparametric, and
decision-theoretic literatures and <strong>novel contributions</strong>
of this work. The DPMirt package synthesizes these components into a
unified computational framework; the novelty lies primarily in the
integration and in extending the CB and GR estimators to the
semiparametric IRT setting.</p>
<blockquote>
<p><strong>Note:</strong> This vignette contains no live MCMC
computation. All code blocks show model specifications and mathematical
formulas for reference. For applied examples with actual model fitting,
see the companion vignettes <code>models-and-workflow</code> and
<code>posterior-summaries</code>.</p>
</blockquote>
</div>
<div class="section level2">
<h2 id="the-bayesian-rasch-model">2. The Bayesian Rasch Model<a class="anchor" aria-label="anchor" href="#the-bayesian-rasch-model"></a>
</h2>
<div class="section level3">
<h3 id="measurement-model">2.1 Measurement Model<a class="anchor" aria-label="anchor" href="#measurement-model"></a>
</h3>
<p>The Rasch model (Rasch, 1960) specifies the probability that person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>p</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(p = 1, \ldots, N)</annotation></semantics></math>
endorses item
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>I</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i = 1, \ldots, I)</annotation></semantics></math>
as a function of two parameters: a person ability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>
and an item difficulty
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>.</p>
<p>The measurement model is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>‚àº</mo><mtext mathvariant="normal">Bernoulli</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
y_{ip} \sim \text{Bernoulli}(\pi_{ip}),
</annotation></semantics></math> where the item response function (IRF)
maps the latent linear predictor to the probability scale via the
logistic link:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">logit</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{logit}(\pi_{ip}) = \theta_p - \beta_i.
</annotation></semantics></math></p>
<p>Equivalently, the probability of a correct response is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>=</mo><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>‚à£</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>,</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\pi_{ip} = P(y_{ip} = 1 \mid \theta_p, \beta_i) =
\frac{\exp(\theta_p - \beta_i)}{1 + \exp(\theta_p - \beta_i)}.
</annotation></semantics></math></p>
<p>The Rasch model embodies the principle of <em>specific
objectivity</em>: the difference between any two persons‚Äô abilities can
be estimated independently of which items are administered, and
conversely for item difficulties.</p>
<p>In NIMBLE, this measurement model is expressed as:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">I</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">y</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="fu">dbern</span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/nimble/man/nimble-math.html" class="external-link">logit</a></span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">eta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">-</span> <span class="va">beta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="sufficient-statistics">2.2 Sufficient Statistics<a class="anchor" aria-label="anchor" href="#sufficient-statistics"></a>
</h3>
<p>A fundamental property of the Rasch model is that the total score
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>p</mi></msub><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></msubsup><msub><mi>y</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">r_p = \sum_{i=1}^{I} y_{ip}</annotation></semantics></math>
is a <strong>sufficient statistic</strong> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>.
The joint likelihood factorizes as:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi mathvariant="bold-italic">ùíö</mi><mi>p</mi></msub><mo>‚à£</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>,</mo><mi mathvariant="bold-italic">ùú∑</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>r</mi><mi>p</mi></msub><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><munder><mo>‚àë</mo><mi>i</mi></munder><msub><mi>y</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munderover><mo>‚àè</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi mathvariant="normal">exp</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
P(\mathbf{y}_p \mid \theta_p, \boldsymbol{\beta}) =
\frac{\exp\left(r_p \theta_p - \sum_i y_{ip} \beta_i\right)}
     {\prod_{i=1}^{I} \left(1 + \exp(\theta_p - \beta_i)\right)},
</annotation></semantics></math> which, by the Neyman-Fisher
factorization theorem, establishes that the posterior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>
depends on the data only through
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mi>p</mi></msub><annotation encoding="application/x-tex">r_p</annotation></semantics></math>.</p>
<blockquote>
<p><strong>Key insight:</strong> The Rasch model is the <em>only</em>
standard unidimensional IRT model with this sufficiency property. Two
persons with the same total score have identical likelihoods for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="hierarchical-framework">2.3 Hierarchical Framework<a class="anchor" aria-label="anchor" href="#hierarchical-framework"></a>
</h3>
<p>The full Bayesian Rasch model has three levels:</p>
<p><strong>Level 1 (Measurement model):</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi mathvariant="bold-italic">ùíÄ</mi><mo>‚à£</mo><mi mathvariant="bold-italic">ùúΩ</mi><mo>,</mo><mi mathvariant="bold-italic">ùú∑</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>‚àè</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><munderover><mo>‚àè</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><msubsup><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow><msub><mi>y</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub></msubsup><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><msup><mo stretchy="false" form="postfix">)</mo><mrow><mn>1</mn><mo>‚àí</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
p(\mathbf{Y} \mid \boldsymbol{\theta}, \boldsymbol{\beta}) =
\prod_{p=1}^{N} \prod_{i=1}^{I}
\pi_{ip}^{y_{ip}} (1 - \pi_{ip})^{1 - y_{ip}},
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{ip}</annotation></semantics></math>
is defined by the Rasch IRF above.</p>
<p><strong>Level 2 (Population model):</strong> Person abilities are
drawn from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mover><mo>‚àº</mo><mrow><mi>i</mi><mi>i</mi><mi>d</mi></mrow></mover><mi>G</mi></mrow><annotation encoding="application/x-tex">\theta_p \stackrel{iid}{\sim} G</annotation></semantics></math>
and item difficulties from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>i</mi></msub><mover><mo>‚àº</mo><mrow><mi>i</mi><mi>i</mi><mi>d</mi></mrow></mover><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œº</mi><mi>Œ≤</mi></msub><mo>,</mo><msubsup><mi>œÉ</mi><mi>Œ≤</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\beta_i \stackrel{iid}{\sim} \text{N}(\mu_\beta, \sigma^2_\beta)</annotation></semantics></math>.
The choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
is where the two approaches diverge: under the parametric prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œº</mi><mi>Œ∏</mi></msub><mo>,</mo><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G = \text{N}(\mu_\theta, \sigma^2_\theta)</annotation></semantics></math>,
and under the DPM prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>‚àº</mo><mtext mathvariant="normal">DP</mtext><mo stretchy="false" form="prefix">(</mo><mi>Œ±</mi><mo>,</mo><msub><mi>G</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G \sim \text{DP}(\alpha, G_0)</annotation></semantics></math>
(Section 5).</p>
<p><strong>Level 3 (Hyperpriors):</strong> For the Normal prior,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œº</mi><mi>Œ∏</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mu_\theta \sim \text{N}(0, 3)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mo>‚àº</mo><mtext mathvariant="normal">Inv-Gamma</mtext><mo stretchy="false" form="prefix">(</mo><mn>2.01</mn><mo>,</mo><mn>1.01</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sigma^2_\theta \sim \text{Inv-Gamma}(2.01, 1.01)</annotation></semantics></math>.
The DPM hyperprior structure is detailed in Section 5.</p>
</div>
<div class="section level3">
<h3 id="posterior-mean-and-shrinkage">2.4 Posterior Mean and Shrinkage<a class="anchor" aria-label="anchor" href="#posterior-mean-and-shrinkage"></a>
</h3>
<p>Under a Normal population prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œº</mi><mi>Œ∏</mi></msub><mo>,</mo><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G = \text{N}(\mu_\theta, \sigma^2_\theta)</annotation></semantics></math>,
the posterior mean (expected a posteriori, or EAP) estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>
exhibits <strong>shrinkage</strong> toward the population mean.</p>
<p>The EAP can be approximately characterized as a weighted combination
of the likelihood-based estimate and the prior mean:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">EAP</mtext></msubsup><mo>‚âà</mo><msub><mi>w</mi><mi>p</mi></msub><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">MLE</mtext></msubsup><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>w</mi><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo><msub><mi>Œº</mi><mi>Œ∏</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{\theta}_p^{\text{EAP}} \approx w_p \hat{\theta}_p^{\text{MLE}} +
(1 - w_p) \mu_\theta,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">MLE</mtext></msubsup><annotation encoding="application/x-tex">\hat{\theta}_p^{\text{MLE}}</annotation></semantics></math>
is the maximum likelihood estimate and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>p</mi></msub><annotation encoding="application/x-tex">w_p</annotation></semantics></math>
is the <strong>shrinkage weight</strong>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>p</mi></msub><mo>=</mo><mfrac><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mrow><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mo>+</mo><mtext mathvariant="normal">se</mtext><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
w_p = \frac{\sigma^2_\theta}{\sigma^2_\theta + \text{se}(\hat{\theta}_p)^2}.
</annotation></semantics></math></p>
<p>Here
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">se</mtext><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mo>=</mo><mn>1</mn><mi>/</mi><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\text{se}(\hat{\theta}_p)^2 = 1 / I(\hat{\theta}_p)</annotation></semantics></math>
is the squared standard error. Key consequences: persons with extreme
scores are pulled more strongly toward
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œº</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\mu_\theta</annotation></semantics></math>
(larger standard errors); the weight
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>p</mi></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">w_p \in (0, 1)</annotation></semantics></math>
is person-specific; and more items mean less shrinkage.</p>
<blockquote>
<p><strong>Key insight:</strong> Shrinkage produces
<strong>underdispersion</strong> of the posterior means relative to the
true population distribution. The variance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">EAP</mtext></msubsup><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\hat{\theta}_p^{\text{EAP}}\}_{p=1}^N</annotation></semantics></math>
is strictly less than the population variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_\theta</annotation></semantics></math>.
This underdispersion is the primary motivation for the CB and GR
estimators (Section 7).</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="test-reliability">2.5 Test Reliability<a class="anchor" aria-label="anchor" href="#test-reliability"></a>
</h3>
<p>The average shrinkage weight across persons provides a measure of
test reliability analogous to classical reliability coefficients:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>w</mi><mo accent="true">‚Äæ</mo></mover><mo>=</mo><mfrac><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mrow><msubsup><mi>œÉ</mi><mi>Œ∏</mi><mn>2</mn></msubsup><mo>+</mo><mover><mtext mathvariant="normal">MSEM</mtext><mo accent="true">¬Ø</mo></mover></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
\bar{w} = \frac{\sigma^2_\theta}{\sigma^2_\theta + \overline{\text{MSEM}}},
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mtext mathvariant="normal">MSEM</mtext><mo accent="true">¬Ø</mo></mover><annotation encoding="application/x-tex">\overline{\text{MSEM}}</annotation></semantics></math>
is the mean squared error of measurement averaged across persons.</p>
<p>The <strong>Fisher information</strong> for person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
under the Rasch model is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
I(\theta_p) = \sum_{i=1}^{I} P_i(\theta_p)(1 - P_i(\theta_p)),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mtext mathvariant="normal">logistic</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P_i(\theta_p) = \text{logistic}(\theta_p - \beta_i)</annotation></semantics></math>
is the probability of correct response. The information is maximized
when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>=</mo><msub><mi>Œ≤</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\theta_p = \beta_i</annotation></semantics></math>
(i.e., when person ability matches item difficulty) and equals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>/</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">I/4</annotation></semantics></math>
when all items have the same difficulty as the person‚Äôs ability.</p>
<p>This connects to the Rasch separation index:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mfrac><msub><mtext mathvariant="normal">SD</mtext><mtext mathvariant="normal">adjusted</mtext></msub><mtext mathvariant="normal">RMSE</mtext></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
G = \frac{\text{SD}_{\text{adjusted}}}{\text{RMSE}},
</annotation></semantics></math> and the associated strata count
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>4</mn><mi>G</mi><mo>+</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">H = (4G + 1)/3</annotation></semantics></math>,
which estimates the number of statistically distinguishable ability
levels the test can discriminate.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-2pl-and-3pl-extensions">3. The 2PL and 3PL Extensions<a class="anchor" aria-label="anchor" href="#the-2pl-and-3pl-extensions"></a>
</h2>
<div class="section level3">
<h3 id="two-parameter-logistic-model-2pl-irt">3.1 Two-Parameter Logistic Model (2PL IRT)<a class="anchor" aria-label="anchor" href="#two-parameter-logistic-model-2pl-irt"></a>
</h3>
<p>The 2PL model (Birnbaum, 1968) extends the Rasch model by introducing
item-specific discrimination parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_i &gt; 0</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">logit</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Œª</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{logit}(\pi_{ip}) = \lambda_i (\theta_p - \beta_i).
</annotation></semantics></math></p>
<p>The discrimination parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œª</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math>
controls the steepness of the item characteristic curve (ICC). Items
with higher
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œª</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math>
are better at distinguishing between persons near the difficulty level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>.</p>
<p>In NIMBLE:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">I</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">y</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="fu">dbern</span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/nimble/man/nimble-math.html" class="external-link">logit</a></span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="va">eta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">-</span> <span class="va">beta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">I</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="fl">0.5</span>, var <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span>  <span class="va">beta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="fl">0</span>, var <span class="op">=</span> <span class="va">sigma2_beta</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œª</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math>
is constrained to be positive via the log-normal prior:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Œª</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><mn>0.5</mn><mo>,</mo><mn>0.5</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log(\lambda_i) \sim \text{N}(0.5, 0.5)</annotation></semantics></math>.
This prior places the median discrimination at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">exp</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><mn>0.5</mn><mo stretchy="false" form="postfix">)</mo><mo>‚âà</mo><mn>1.65</mn></mrow><annotation encoding="application/x-tex">\exp(0.5) \approx 1.65</annotation></semantics></math>
with reasonable spread over practically relevant values.</p>
<blockquote>
<p><strong>Key difference from Rasch:</strong> The total score
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mi>p</mi></msub><annotation encoding="application/x-tex">r_p</annotation></semantics></math>
is no longer sufficient for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>
in the 2PL model. The specific pattern of correct and incorrect
responses matters because items contribute differentially to the
likelihood.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="slope-intercept-si-parameterization">3.2 Slope-Intercept (SI) Parameterization<a class="anchor" aria-label="anchor" href="#slope-intercept-si-parameterization"></a>
</h3>
<p>An alternative parameterization of the 2PL replaces
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Œª</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_i, \beta_i)</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>Œª</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ≥</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_i, \gamma_i)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≥</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\gamma_i</annotation></semantics></math>
is the intercept:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">logit</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Œ≥</mi><mi>i</mi></msub><mo>+</mo><msub><mi>Œª</mi><mi>i</mi></msub><msub><mi>Œ∏</mi><mi>p</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{logit}(\pi_{ip}) = \gamma_i + \lambda_i \theta_p.
</annotation></semantics></math></p>
<p>The two parameterizations are related by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≥</mi><mi>i</mi></msub><mo>=</mo><mi>‚àí</mi><msub><mi>Œª</mi><mi>i</mi></msub><msub><mi>Œ≤</mi><mi>i</mi></msub><mspace width="1.0em"></mspace><mo>‚áî</mo><mspace width="1.0em"></mspace><msub><mi>Œ≤</mi><mi>i</mi></msub><mo>=</mo><mi>‚àí</mi><msub><mi>Œ≥</mi><mi>i</mi></msub><mi>/</mi><msub><mi>Œª</mi><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
\gamma_i = -\lambda_i \beta_i \quad \Longleftrightarrow \quad
\beta_i = -\gamma_i / \lambda_i.
</annotation></semantics></math></p>
<p>In NIMBLE:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">I</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">y</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="fu">dbern</span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/nimble/man/nimble-math.html" class="external-link">logit</a></span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="va">eta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">+</span> <span class="va">gamma</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>The SI parameterization is sometimes preferred for computational
reasons because the linear predictor is additive in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>,
which can improve mixing in certain MCMC samplers. DPMirt supports both
parameterizations.</p>
</div>
<div class="section level3">
<h3 id="three-parameter-logistic-model-3pl">3.3 Three-Parameter Logistic Model (3PL)<a class="anchor" aria-label="anchor" href="#three-parameter-logistic-model-3pl"></a>
</h3>
<p>The 3PL model (Birnbaum, 1968) adds a lower asymptote parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ¥</mi><mi>i</mi></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\delta_i \in [0, 1]</annotation></semantics></math>
representing the probability that a person with very low ability still
endorses the item (the ‚Äúguessing‚Äù parameter):
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>Œ¥</mi><mi>i</mi></msub><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>Œ¥</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚ãÖ</mo><mtext mathvariant="normal">logistic</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œª</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\pi_{ip} = \delta_i + (1 - \delta_i) \cdot
\text{logistic}(\lambda_i (\theta_p - \beta_i)).
</annotation></semantics></math></p>
<p>The item response probability approaches
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ¥</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\delta_i</annotation></semantics></math>
(rather than 0) as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚Üí</mo><mi>‚àí</mi><mi>‚àû</mi></mrow><annotation encoding="application/x-tex">\theta_p \to -\infty</annotation></semantics></math>.
The upper asymptote remains at 1.</p>
<p>In NIMBLE:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">I</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">y</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="fu">dbern</span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">delta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">delta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="va">linearReg</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/nimble/man/nimble-math.html" class="external-link">logit</a></span><span class="op">(</span><span class="va">linearReg</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="va">eta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">-</span> <span class="va">beta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">I</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">delta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">dbeta</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">12</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>The prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ¥</mi><mi>i</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">Beta</mtext><mo stretchy="false" form="prefix">(</mo><mn>4</mn><mo>,</mo><mn>12</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\delta_i \sim \text{Beta}(4, 12)</annotation></semantics></math>
has mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>/</mi><mn>16</mn><mo>=</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">4/16 = 0.25</annotation></semantics></math>
and concentrates most mass between 0.05 and 0.45, reflecting the prior
belief that guessing parameters are typically modest. This is a standard
informative prior for the lower asymptote (De Ayala, 2022).</p>
</div>
</div>
<div class="section level2">
<h2 id="identification-and-rescaling">4. Identification and Rescaling<a class="anchor" aria-label="anchor" href="#identification-and-rescaling"></a>
</h2>
<div class="section level3">
<h3 id="the-identification-problem">4.1 The Identification Problem<a class="anchor" aria-label="anchor" href="#the-identification-problem"></a>
</h3>
<p>IRT models contain <strong>identification indeterminacy</strong>:
certain transformations of the parameters leave the likelihood
invariant. Without constraints, the posterior is improper along these
directions.</p>
<p><strong>Rasch model (location indeterminacy):</strong> For any
constant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>,
the transformation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚Ü¶</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">\theta_p \mapsto \theta_p + c</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>i</mi></msub><mo>‚Ü¶</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">\beta_i \mapsto \beta_i + c</annotation></semantics></math>
leaves
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\theta_p - \beta_i</annotation></semantics></math>
unchanged and hence the likelihood invariant. Only one degree of freedom
(location) is indeterminate.</p>
<p><strong>2PL and 3PL models (location + scale indeterminacy):</strong>
For constants
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d &gt; 0</annotation></semantics></math>,
the transformation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚Ü¶</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>+</mo><mi>c</mi><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">\theta_p \mapsto (\theta_p + c) / d</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>i</mi></msub><mo>‚Ü¶</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo>+</mo><mi>c</mi><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">\beta_i \mapsto (\beta_i + c) / d</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>i</mi></msub><mo>‚Ü¶</mo><msub><mi>Œª</mi><mi>i</mi></msub><mo>‚ãÖ</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\lambda_i \mapsto \lambda_i \cdot d</annotation></semantics></math>
leaves
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\lambda_i(\theta_p - \beta_i)</annotation></semantics></math>
invariant. Two degrees of freedom (location and scale) are
indeterminate.</p>
<p>The choice of identification strategy affects both the interpretation
of parameters and the efficiency of MCMC sampling.</p>
</div>
<div class="section level3">
<h3 id="three-identification-strategies">4.2 Three Identification Strategies<a class="anchor" aria-label="anchor" href="#three-identification-strategies"></a>
</h3>
<p>DPMirt supports three identification strategies, selected via the
<code>identification</code> argument to <code><a href="../reference/dpmirt_spec.html">dpmirt_spec()</a></code>:</p>
<p><strong>Strategy 1: <code>constrained_ability</code></strong> ‚Äì Fix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi><mo>‚àº</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\theta \sim \text{N}(0, 1)</annotation></semantics></math>.
Resolves both location and scale indeterminacy but constrains the latent
distribution and is incompatible with the DPM prior.</p>
<p><strong>Strategy 2: <code>constrained_item</code></strong> ‚Äì Center
item parameters during MCMC. For Rasch:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ≤</mi><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><msubsup><mi>Œ≤</mi><mi>i</mi><mtext mathvariant="normal">tmp</mtext></msubsup><mo>‚àí</mo><mfrac><mn>1</mn><mi>I</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><msubsup><mi>Œ≤</mi><mi>j</mi><mtext mathvariant="normal">tmp</mtext></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\beta_i^* = \beta_i^{\text{tmp}} - \frac{1}{I}\sum_{j=1}^{I} \beta_j^{\text{tmp}}.
</annotation></semantics></math></p>
<p>For 2PL/3PL, both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
(or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>)
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>Œª</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\log(\lambda)</annotation></semantics></math>
are centered:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ≤</mi><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><msubsup><mi>Œ≤</mi><mi>i</mi><mtext mathvariant="normal">tmp</mtext></msubsup><mo>‚àí</mo><msup><mover><mi>Œ≤</mi><mo accent="true">‚Äæ</mo></mover><mtext mathvariant="normal">tmp</mtext></msup><mo>,</mo><mspace width="1.0em"></mspace><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œª</mi><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œª</mi><mi>i</mi><mtext mathvariant="normal">tmp</mtext></msubsup><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msup><mover><mrow><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>Œª</mi></mrow><mo accent="true">¬Ø</mo></mover><mtext mathvariant="normal">tmp</mtext></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\beta_i^* = \beta_i^{\text{tmp}} - \bar{\beta}^{\text{tmp}}, \quad
\log(\lambda_i^*) = \log(\lambda_i^{\text{tmp}}) - \overline{\log \lambda}^{\text{tmp}}.
</annotation></semantics></math></p>
<p>This approach identifies the model within the MCMC without
constraining the ability distribution, making it compatible with both
Normal and DPM priors.</p>
<p><strong>Strategy 3: <code>unconstrained</code></strong> ‚Äì Place no
identification constraints during MCMC. Instead, apply post-hoc
rescaling to the posterior samples to achieve identification. This
approach, advocated by Paganin et al. (2023), has the advantage of not
interfering with the sampler‚Äôs geometry.</p>
</div>
<div class="section level3">
<h3 id="post-hoc-rescaling-formulas">4.3 Post-hoc Rescaling Formulas<a class="anchor" aria-label="anchor" href="#post-hoc-rescaling-formulas"></a>
</h3>
<p>For <strong>unconstrained</strong> models, rescaling is applied
iteration-by-iteration to the MCMC output. Let superscript
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(s)</annotation></semantics></math>
denote MCMC iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>.</p>
<p><strong>Rasch model (location shift only):</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ≤</mi><mi>i</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>Œ≤</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚àí</mo><msup><mover><mi>Œ≤</mi><mo accent="true">‚Äæ</mo></mover><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="1.0em"></mspace><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚àí</mo><msup><mover><mi>Œ≤</mi><mo accent="true">‚Äæ</mo></mover><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\beta_i^{*(s)} = \beta_i^{(s)} - \bar{\beta}^{(s)}, \quad
\theta_p^{*(s)} = \theta_p^{(s)} - \bar{\beta}^{(s)},
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>Œ≤</mi><mo accent="true">‚Äæ</mo></mover><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mi>I</mi></mfrac><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></msubsup><msubsup><mi>Œ≤</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\bar{\beta}^{(s)} = \frac{1}{I}\sum_{i=1}^{I} \beta_i^{(s)}</annotation></semantics></math>
is the mean item difficulty at iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>.</p>
<p><strong>2PL/3PL IRT parameterization (location + scale):</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mover><mi>Œ≤</mi><mo accent="true">‚Äæ</mo></mover><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="1.0em"></mspace><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>‚àè</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><msubsup><mi>Œª</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>‚àí</mi><mn>1</mn><mi>/</mi><mi>I</mi></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
c^{(s)} = \bar{\beta}^{(s)}, \quad
d^{(s)} = \left(\prod_{i=1}^{I} \lambda_i^{(s)}\right)^{-1/I},
</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ≤</mi><mi>i</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><msubsup><mi>Œ≤</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚àí</mo><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mrow><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mfrac><mo>,</mo><mspace width="1.0em"></mspace><msubsup><mi>Œª</mi><mi>i</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>Œª</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚ãÖ</mo><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="1.0em"></mspace><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚àí</mo><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mrow><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\beta_i^{*(s)} = \frac{\beta_i^{(s)} - c^{(s)}}{d^{(s)}}, \quad
\lambda_i^{*(s)} = \lambda_i^{(s)} \cdot d^{(s)}, \quad
\theta_p^{*(s)} = \frac{\theta_p^{(s)} - c^{(s)}}{d^{(s)}}.
</annotation></semantics></math></p>
<p>The scale factor
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">d^{(s)}</annotation></semantics></math>
is the inverse geometric mean of the discriminations, ensuring that the
geometric mean of the rescaled
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Œª</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\lambda^*</annotation></semantics></math>
equals 1.</p>
<p><strong>2PL/3PL SI parameterization:</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>=</mo><mfrac><mrow><munder><mo>‚àë</mo><mi>i</mi></munder><msubsup><mi>Œ≥</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup></mrow><mrow><munder><mo>‚àë</mo><mi>i</mi></munder><msubsup><mi>Œª</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup></mrow></mfrac><mo>,</mo><mspace width="1.0em"></mspace><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>‚àè</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><msubsup><mi>Œª</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>‚àí</mi><mn>1</mn><mi>/</mi><mi>I</mi></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
c^{(s)} = \frac{\sum_i \gamma_i^{(s)}}{\sum_i \lambda_i^{(s)}}, \quad
d^{(s)} = \left(\prod_{i=1}^{I} \lambda_i^{(s)}\right)^{-1/I},
</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ≥</mi><mi>i</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>Œ≥</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚àí</mo><msubsup><mi>Œª</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚ãÖ</mo><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="1.0em"></mspace><msubsup><mi>Œª</mi><mi>i</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>Œª</mi><mi>i</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>‚ãÖ</mo><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="1.0em"></mspace><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mi>*</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>+</mo><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mrow><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\gamma_i^{*(s)} = \gamma_i^{(s)} - \lambda_i^{(s)} \cdot c^{(s)}, \quad
\lambda_i^{*(s)} = \lambda_i^{(s)} \cdot d^{(s)}, \quad
\theta_p^{*(s)} = \frac{\theta_p^{(s)} + c^{(s)}}{d^{(s)}}.
</annotation></semantics></math></p>
<p>Note the sign difference for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
in the SI case: because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≥</mi><mi>i</mi></msub><mo>=</mo><mi>‚àí</mi><msub><mi>Œª</mi><mi>i</mi></msub><msub><mi>Œ≤</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\gamma_i = -\lambda_i \beta_i</annotation></semantics></math>,
the location shift has the opposite sign compared to the IRT
parameterization.</p>
<blockquote>
<p><strong>Key insight:</strong> The 3PL guessing parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ¥</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\delta_i</annotation></semantics></math>
is scale-invariant and does not require rescaling. It enters the model
multiplicatively outside the logistic function and is unaffected by
affine transformations of the ability scale.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="efficiency-of-unconstrained-sampling">4.4 Efficiency of Unconstrained Sampling<a class="anchor" aria-label="anchor" href="#efficiency-of-unconstrained-sampling"></a>
</h3>
<p>Paganin et al.¬†(2023) found that unconstrained sampling with post-hoc
rescaling is generally the most efficient strategy, because constraining
parameters during sampling can distort the posterior geometry. DPMirt
uses unconstrained identification as the default for 2PL and 3PL models,
and constrained_item as the default for Rasch (where the difference is
smaller).</p>
</div>
</div>
<div class="section level2">
<h2 id="dirichlet-process-mixture-priors">5. Dirichlet Process Mixture Priors<a class="anchor" aria-label="anchor" href="#dirichlet-process-mixture-priors"></a>
</h2>
<div class="section level3">
<h3 id="the-dirichlet-process">5.1 The Dirichlet Process<a class="anchor" aria-label="anchor" href="#the-dirichlet-process"></a>
</h3>
<p>The Dirichlet Process (DP; Ferguson, 1973) is a distribution over
probability distributions. A random measure
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>‚àº</mo><mtext mathvariant="normal">DP</mtext><mo stretchy="false" form="prefix">(</mo><mi>Œ±</mi><mo>,</mo><msub><mi>G</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G \sim \text{DP}(\alpha, G_0)</annotation></semantics></math>
satisfies: for every finite measurable partition
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>A</mi><mi>m</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(A_1, \ldots, A_m)</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>A</mi><mi>m</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mtext mathvariant="normal">Dirichlet</mtext><mo stretchy="false" form="prefix">(</mo><mi>Œ±</mi><msub><mi>G</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>Œ±</mi><msub><mi>G</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>A</mi><mi>m</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
(G(A_1), \ldots, G(A_m)) \sim
\text{Dirichlet}(\alpha G_0(A_1), \ldots, \alpha G_0(A_m)).
</annotation></semantics></math></p>
<p>Three properties are essential: (1) <strong>Almost-sure
discreteness</strong> ‚Äì draws from the DP are discrete with probability
one, inducing clustering. (2) <strong>Centering at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>G</mi><mn>0</mn></msub><annotation encoding="application/x-tex">G_0</annotation></semantics></math></strong>
‚Äì
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msub><mi>G</mi><mn>0</mn></msub><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[G(A)] = G_0(A)</annotation></semantics></math>
for any measurable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
(3) <strong>Concentration control</strong> ‚Äì as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚Üí</mo><mi>‚àû</mi></mrow><annotation encoding="application/x-tex">\alpha \to \infty</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>‚Üí</mo><msub><mi>G</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">G \to G_0</annotation></semantics></math>;
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚Üí</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha \to 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
concentrates on a single atom.</p>
</div>
<div class="section level3">
<h3 id="the-chinese-restaurant-process">5.2 The Chinese Restaurant Process<a class="anchor" aria-label="anchor" href="#the-chinese-restaurant-process"></a>
</h3>
<p>The Chinese Restaurant Process (CRP; Blackwell &amp; MacQueen, 1973)
characterizes the partition structure induced by the DP. Customers
(persons) arrive sequentially at a restaurant with infinitely many
tables (clusters). Customer
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
joins existing table
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
with probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>k</mi></msub><mi>/</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ±</mi><mo>+</mo><mi>p</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">n_k / (\alpha + p - 1)</annotation></semantics></math>
or starts a new table with probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mi>/</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ±</mi><mo>+</mo><mi>p</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\alpha / (\alpha + p - 1)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>k</mi></msub><annotation encoding="application/x-tex">n_k</annotation></semantics></math>
is the current table size.</p>
<blockquote>
<p><strong>Key insight:</strong> The CRP exhibits a ‚Äúrich get richer‚Äù
property: tables with more customers are proportionally more likely to
attract new customers. This produces a power-law distribution of cluster
sizes and explains why DP mixtures naturally create a few large clusters
and many small ones.</p>
</blockquote>
<p>The CRP partition is <strong>exchangeable</strong> (independent of
arrival order), enabling Gibbs-based MCMC. The expected number of
clusters is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>K</mi><mi>N</mi></msub><mo>‚à£</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">]</mo><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mi>Œ±</mi><mrow><mi>Œ±</mi><mo>+</mo><mi>p</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>‚âà</mo><mi>Œ±</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>N</mi><mo stretchy="false" form="postfix">)</mo><mspace width="1.0em"></mspace><mrow><mtext mathvariant="normal">for large </mtext><mspace width="0.333em"></mspace></mrow><mi>N</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\mathbb{E}[K_N \mid \alpha] = \sum_{p=1}^{N} \frac{\alpha}{\alpha + p - 1}
\approx \alpha \log(N) \quad \text{for large } N.
</annotation></semantics></math></p>
<p>This logarithmic growth is a reasonable modeling assumption for
latent trait distributions.</p>
</div>
<div class="section level3">
<h3 id="dpm-irt-model-specification">5.3 DPM-IRT Model Specification<a class="anchor" aria-label="anchor" href="#dpm-irt-model-specification"></a>
</h3>
<p>The DPM-IRT model replaces the parametric Normal prior with a DP
Mixture. The measurement model is unchanged; the DPM prior specifies:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>p</mi></msub><mo>,</mo><mover><mi mathvariant="bold-italic">ùùÅ</mi><mo accent="true">ÃÉ</mo></mover><mo>,</mo><msup><mover><mi mathvariant="bold-italic">ùùà</mi><mo accent="true">ÃÉ</mo></mover><mn>2</mn></msup><mo>‚àº</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œº</mi><mo accent="true">ÃÉ</mo></mover><msub><mi>z</mi><mi>p</mi></msub></msub><mo>,</mo><msubsup><mover><mi>œÉ</mi><mo accent="true">ÃÉ</mo></mover><msub><mi>z</mi><mi>p</mi></msub><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
\theta_p \mid z_p, \tilde{\boldsymbol{\mu}}, \tilde{\boldsymbol{\sigma}}^2
\sim \text{N}(\tilde{\mu}_{z_p}, \tilde{\sigma}^2_{z_p}),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>p</mi></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>‚Ä¶</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">z_p \in \{1, 2, \ldots\}</annotation></semantics></math>
is the cluster assignment for person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>.</p>
<p><strong>CRP prior for cluster assignments:</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">ùíõ</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àº</mo><mtext mathvariant="normal">CRP</mtext><mo stretchy="false" form="prefix">(</mo><mi>Œ±</mi><mo>,</mo><mi>N</mi><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\mathbf{z} = (z_1, \ldots, z_N) \sim \text{CRP}(\alpha, N).
</annotation></semantics></math></p>
<p><strong>Base measure for cluster parameters:</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>œÉ</mi><mi>Œº</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo><mo>√ó</mo><mtext mathvariant="normal">Inv-Gamma</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>ŒΩ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>ŒΩ</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
G_0 = \text{N}(0, \sigma^2_\mu) \times \text{Inv-Gamma}(\nu_1, \nu_2),
</annotation></semantics></math> meaning that for each cluster
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>Œº</mi><mo accent="true">ÃÉ</mo></mover><mi>m</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">N</mtext><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>œÉ</mi><mi>Œº</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="1.0em"></mspace><msubsup><mover><mi>œÉ</mi><mo accent="true">ÃÉ</mo></mover><mi>m</mi><mn>2</mn></msubsup><mo>‚àº</mo><mtext mathvariant="normal">Inv-Gamma</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>ŒΩ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>ŒΩ</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\tilde{\mu}_m \sim \text{N}(0, \sigma^2_\mu), \quad
\tilde{\sigma}^2_m \sim \text{Inv-Gamma}(\nu_1, \nu_2).
</annotation></semantics></math></p>
<p><strong>Default hyperparameters</strong> (following Paganin et al.,
2023):
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>œÉ</mi><mi>Œº</mi><mn>2</mn></msubsup><mo>=</mo><mn>2</mn><mo>,</mo><mspace width="1.0em"></mspace><msub><mi>ŒΩ</mi><mn>1</mn></msub><mo>=</mo><mn>2.01</mn><mo>,</mo><mspace width="1.0em"></mspace><msub><mi>ŒΩ</mi><mn>2</mn></msub><mo>=</mo><mn>1.01</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">
\sigma^2_\mu = 2, \quad \nu_1 = 2.01, \quad \nu_2 = 1.01.
</annotation></semantics></math></p>
<p>These values place the cluster variance prior just above the boundary
for finite mean
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ŒΩ</mi><mn>1</mn></msub><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\nu_1 &gt; 2</annotation></semantics></math>)
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msup><mover><mi>œÉ</mi><mo accent="true">ÃÉ</mo></mover><mn>2</mn></msup><mo stretchy="false" form="postfix">]</mo><mo>‚âà</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathbb{E}[\tilde{\sigma}^2] \approx 1</annotation></semantics></math>.</p>
<p>In NIMBLE, the DPM prior is implemented via the CRP
representation:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># CRP cluster assignments</span></span>
<span><span class="va">zi</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">]</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/nimble/man/ChineseRestaurantProcess.html" class="external-link">dCRP</a></span><span class="op">(</span><span class="va">alpha</span>, size <span class="op">=</span> <span class="va">N</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Concentration parameter hyperprior</span></span>
<span><span class="va">alpha</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">dgamma</a></span><span class="op">(</span><span class="va">a</span>, <span class="va">b</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Person abilities: Normal kernel with cluster-specific parameters</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">eta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="va">mu_j</span><span class="op">[</span><span class="va">j</span><span class="op">]</span>, var <span class="op">=</span> <span class="va">s2_j</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">mu_j</span><span class="op">[</span><span class="va">j</span><span class="op">]</span>  <span class="op">&lt;-</span> <span class="va">muTilde</span><span class="op">[</span><span class="va">zi</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="va">s2_j</span><span class="op">[</span><span class="va">j</span><span class="op">]</span>  <span class="op">&lt;-</span> <span class="va">s2Tilde</span><span class="op">[</span><span class="va">zi</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Cluster parameters drawn from base measure G0</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">m</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">M</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">muTilde</span><span class="op">[</span><span class="va">m</span><span class="op">]</span>  <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="fl">0</span>, var <span class="op">=</span> <span class="va">s2_mu</span><span class="op">)</span></span>
<span>  <span class="va">s2Tilde</span><span class="op">[</span><span class="va">m</span><span class="op">]</span>  <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/nimble/man/Inverse-Gamma.html" class="external-link">dinvgamma</a></span><span class="op">(</span><span class="va">nu1</span>, <span class="va">nu2</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="stick-breaking-vs--crp">5.4 Stick-Breaking vs.¬†CRP<a class="anchor" aria-label="anchor" href="#stick-breaking-vs--crp"></a>
</h3>
<p>An alternative to the CRP is Sethuraman‚Äôs (1994)
<strong>stick-breaking</strong> construction, which represents
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
as an infinite discrete measure:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mo accent="false">‚àû</mo></munderover><msub><mi>w</mi><mi>h</mi></msub><msub><mi>Œ¥</mi><msubsup><mi>Œ∏</mi><mi>h</mi><mo>*</mo></msubsup></msub><mo>,</mo><mspace width="1.0em"></mspace><msub><mi>w</mi><mi>h</mi></msub><mo>=</mo><msub><mi>v</mi><mi>h</mi></msub><munder><mo>‚àè</mo><mrow><mi>‚Ñì</mi><mo>&lt;</mo><mi>h</mi></mrow></munder><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>v</mi><mi>‚Ñì</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="1.0em"></mspace><msub><mi>v</mi><mi>h</mi></msub><mover><mo>‚àº</mo><mrow><mi>i</mi><mi>i</mi><mi>d</mi></mrow></mover><mtext mathvariant="normal">Beta</mtext><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>,</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
G = \sum_{h=1}^{\infty} w_h \delta_{\theta^*_h}, \quad
w_h = v_h \prod_{\ell &lt; h}(1 - v_\ell), \quad
v_h \stackrel{iid}{\sim} \text{Beta}(1, \alpha),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msubsup><mi>Œ∏</mi><mi>h</mi><mo>*</mo></msubsup><mo stretchy="false" form="postfix">}</mo><mover><mo>‚àº</mo><mrow><mi>i</mi><mi>i</mi><mi>d</mi></mrow></mover><msub><mi>G</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\{\theta^*_h\} \stackrel{iid}{\sim} G_0</annotation></semantics></math>.
DPMirt uses the <strong>CRP representation</strong> instead, because it
is natively supported by NIMBLE‚Äôs <code>dCRP</code> distribution and
offers direct access to cluster assignments
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding="application/x-tex">z_p</annotation></semantics></math>
without requiring explicit truncation of the number of components.</p>
</div>
<div class="section level3">
<h3 id="practical-truncation">5.5 Practical Truncation<a class="anchor" aria-label="anchor" href="#practical-truncation"></a>
</h3>
<p>Although the CRP is defined over infinitely many potential clusters,
NIMBLE requires a finite upper bound
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
on pre-allocated cluster parameter vectors. The default in DPMirt is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">M = 50</annotation></semantics></math>.
Since
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>K</mi><mi>N</mi></msub><mo>‚à£</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">]</mo><mo>‚âà</mo><mi>Œ±</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[K_N \mid \alpha] \approx \alpha \log(N)</annotation></semantics></math>
and typical applications have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">[</mo><mn>0.5</mn><mo>,</mo><mn>5</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\alpha \in [0.5, 5]</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚â§</mo><mn>2</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">N \leq 2{,}000</annotation></semantics></math>,
the default
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">M = 50</annotation></semantics></math>
is conservative. If the MCMC sampler creates clusters approaching
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>,
the user should increase it via the <code>M</code> argument to
<code><a href="../reference/dpmirt_spec.html">dpmirt_spec()</a></code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-concentration-parameter-alpha">6. The Concentration Parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>)<a class="anchor" aria-label="anchor" href="#the-concentration-parameter-alpha"></a>
</h2>
<div class="section level3">
<h3 id="role-of-alpha">6.1 Role of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#role-of-alpha"></a>
</h3>
<p>The concentration parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
controls (1) the expected number of clusters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>K</mi><mi>N</mi></msub><mo>‚à£</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[K_N \mid \alpha]</annotation></semantics></math>,
(2) the degree of departure from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>G</mi><mn>0</mn></msub><annotation encoding="application/x-tex">G_0</annotation></semantics></math>,
and (3) the resolution of density estimation. For large
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>K</mi><mi>N</mi></msub><mo>‚à£</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">]</mo><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mi>Œ±</mi><mrow><mi>Œ±</mi><mo>+</mo><mi>p</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>‚âà</mo><mi>Œ±</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>N</mi><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\mathbb{E}[K_N \mid \alpha] = \sum_{p=1}^{N} \frac{\alpha}{\alpha + p - 1}
\approx \alpha \log(N).
</annotation></semantics></math></p>
<p>Illustrative values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">N = 500</annotation></semantics></math>:</p>
<table class="table">
<thead><tr class="header">
<th align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></th>
<th align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>K</mi><mi>N</mi></msub><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[K_N]</annotation></semantics></math></th>
<th align="center">Interpretation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">0.5</td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\approx 3</annotation></semantics></math></td>
<td align="center">Few broad clusters</td>
</tr>
<tr class="even">
<td align="center">1.0</td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">\approx 6</annotation></semantics></math></td>
<td align="center">Moderate clustering</td>
</tr>
<tr class="odd">
<td align="center">3.0</td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>19</mn></mrow><annotation encoding="application/x-tex">\approx 19</annotation></semantics></math></td>
<td align="center">Many clusters</td>
</tr>
<tr class="even">
<td align="center">10.0</td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>62</mn></mrow><annotation encoding="application/x-tex">\approx 62</annotation></semantics></math></td>
<td align="center">Near-continuous</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="gamma-hyperprior">6.2 Gamma Hyperprior<a class="anchor" aria-label="anchor" href="#gamma-hyperprior"></a>
</h3>
<p>DPMirt places
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚àº</mo><mtext mathvariant="normal">Gamma</mtext><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\alpha \sim \text{Gamma}(a, b)</annotation></semantics></math>
(shape-rate parameterization,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mi>a</mi><mi>/</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">\mathbb{E}[\alpha] = a/b</annotation></semantics></math>).
The default is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚àº</mo><mtext mathvariant="normal">Gamma</mtext><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\alpha \sim \text{Gamma}(1, 3)</annotation></semantics></math>
following Paganin et al.¬†(2023), with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><mi>Œ±</mi><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mn>1</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">\mathbb{E}[\alpha] = 1/3</annotation></semantics></math>.
This conservative prior favors parsimonious models with few clusters,
allowing data to drive the discovery of multiple subpopulations.</p>
</div>
<div class="section level3">
<h3 id="principled-elicitation-via-dpprior">6.3 Principled Elicitation via DPprior<a class="anchor" aria-label="anchor" href="#principled-elicitation-via-dpprior"></a>
</h3>
<p>The DPprior package (Lee, 2026) translates beliefs about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œº</mi><mi>K</mi></msub><mo>=</mo><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>K</mi><mi>N</mi></msub><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mu_K =
\mathbb{E}[K_N]</annotation></semantics></math> into Gamma parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(a, b)</annotation></semantics></math>
via moment-matching. DPMirt integrates through
<code><a href="../reference/dpmirt_alpha_prior.html">dpmirt_alpha_prior()</a></code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha_ab</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dpmirt_alpha_prior.html">dpmirt_alpha_prior</a></span><span class="op">(</span>N <span class="op">=</span> <span class="fl">500</span>, mu_K <span class="op">=</span> <span class="fl">5</span>, confidence <span class="op">=</span> <span class="st">"medium"</span><span class="op">)</span></span>
<span><span class="va">spec</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dpmirt_spec.html">dpmirt_spec</a></span><span class="op">(</span><span class="va">data</span>, model <span class="op">=</span> <span class="st">"rasch"</span>, prior <span class="op">=</span> <span class="st">"dpm"</span>,</span>
<span>                    alpha_prior <span class="op">=</span> <span class="va">alpha_ab</span><span class="op">)</span></span></code></pre></div>
<p>If DPprior is not installed, DPMirt falls back to Gamma(1, 3).</p>
<blockquote>
<p><strong>Key insight:</strong> The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
hyperprior meaningfully impacts inference for moderate sample sizes.
Confirmatory applications benefit from principled elicitation encoding
domain knowledge about population heterogeneity.</p>
</blockquote>
</div>
</div>
<div class="section level2">
<h2 id="posterior-summary-theory">7. Posterior Summary Theory<a class="anchor" aria-label="anchor" href="#posterior-summary-theory"></a>
</h2>
<p>A central theme of the DPMirt package is that the choice of posterior
summary method should be guided by the inferential goal. Different loss
functions lead to different optimal estimators, and no single estimator
is universally best.</p>
<div class="section level3">
<h3 id="loss-functions-and-optimality">7.1 Loss Functions and Optimality<a class="anchor" aria-label="anchor" href="#loss-functions-and-optimality"></a>
</h3>
<p>Consider
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
persons with true abilities
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Œ∏</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">\theta_1, \ldots, \theta_N</annotation></semantics></math>
and estimates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\theta}_1, \ldots, \hat{\theta}_N</annotation></semantics></math>.
We define three loss functions corresponding to three inferential
goals.</p>
<p><strong>Goal 1: Individual estimation accuracy.</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">MSEL</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo>‚àí</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{MSEL} = \frac{1}{N} \sum_{p=1}^{N}
(\hat{\theta}_p - \theta_p)^2.
</annotation></semantics></math></p>
<p>The posterior mean (PM) minimizes MSEL in expectation under the
posterior. This is the classical Bayes estimator under squared error
loss.</p>
<p><strong>Goal 2: Ranking quality.</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">MSELP</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><msub><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mi>N</mi></mfrac><mo>‚àí</mo><mfrac><msub><mi>R</mi><mi>p</mi></msub><mi>N</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\text{MSELP} = \frac{1}{N} \sum_{p=1}^{N}
\left(\frac{\hat{R}_p}{N} - \frac{R_p}{N}\right)^2,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo>=</mo><mtext mathvariant="normal">rank</mtext><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\hat{R}_p = \text{rank}(\hat{\theta}_p)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>p</mi></msub><mo>=</mo><mtext mathvariant="normal">rank</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">R_p = \text{rank}(\theta_p)</annotation></semantics></math>
are the estimated and true ranks, respectively. This loss measures how
well the estimator preserves the ordering of persons.</p>
<p><strong>Goal 3: Distribution recovery.</strong>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">KS</mtext><mo>=</mo><munder><mi mathvariant="normal">sup</mi><mi>t</mi></munder><mrow><mo stretchy="true" form="prefix">|</mo><msub><mover><mi>G</mi><mo accent="true">ÃÇ</mo></mover><mi>N</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><msub><mi>G</mi><mi>N</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">|</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\text{KS} = \sup_{t} \left|\hat{G}_N(t) - G_N(t)\right|,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>G</mi><mo accent="true">ÃÇ</mo></mover><mi>N</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>‚àë</mo><mi>p</mi></msub><mn mathvariant="bold">ùüè</mn><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo>‚â§</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\hat{G}_N(t) = \frac{1}{N}\sum_p \mathbf{1}(\hat{\theta}_p \leq t)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>N</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>‚àë</mo><mi>p</mi></msub><mn mathvariant="bold">ùüè</mn><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚â§</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">G_N(t) = \frac{1}{N}\sum_p \mathbf{1}(\theta_p \leq t)</annotation></semantics></math>
are the empirical distribution functions of the estimates and true
values. This loss measures how well the set of estimates reproduces the
shape of the true ability distribution.</p>
<blockquote>
<p><strong>Key insight:</strong> The PM estimator is optimal for Goal 1
but performs poorly on Goals 2 and 3 due to shrinkage-induced
underdispersion. The CB estimator corrects the underdispersion (Goal 3),
and the GR estimator simultaneously addresses ranking and distribution
recovery (Goals 2 and 3).</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="posterior-mean-pm">7.2 Posterior Mean (PM)<a class="anchor" aria-label="anchor" href="#posterior-mean-pm"></a>
</h3>
<p>The posterior mean is the most familiar Bayesian point estimate:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">PM</mtext></msubsup><mo>=</mo><mi mathvariant="double-struck">ùîº</mi><mo stretchy="false" form="prefix">[</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚à£</mo><mi mathvariant="bold-italic">ùíÄ</mi><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{\theta}_p^{\text{PM}} = \mathbb{E}[\theta_p \mid \mathbf{Y}] =
\frac{1}{S} \sum_{s=1}^{S} \theta_p^{(s)},
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_p^{(s)}</annotation></semantics></math>
is the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\theta_p</annotation></semantics></math>
at MCMC iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is the total number of post-burnin samples.</p>
<p>The PM minimizes the posterior expected squared error loss for each
person individually. However, the set of posterior means
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">PM</mtext></msubsup><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\hat{\theta}_p^{\text{PM}}\}_{p=1}^N</annotation></semantics></math>
is <strong>underdispersed</strong>: their empirical variance is strictly
less than the posterior expectation of the population variance. This
underdispersion distorts the shape of the estimated ability distribution
and can bias percentile-based inferences.</p>
</div>
<div class="section level3">
<h3 id="constrained-bayes-cb">7.3 Constrained Bayes (CB)<a class="anchor" aria-label="anchor" href="#constrained-bayes-cb"></a>
</h3>
<p>The Constrained Bayes estimator, introduced by Ghosh (1992) and
developed further by Louis (1984) and Ghosh and Kim (2002), addresses
the underdispersion of the posterior mean by imposing two
constraints:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Match the marginal mean:</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>‚àë</mo><mi>p</mi></msub><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">CB</mtext></msubsup><mo>=</mo><mover><mi>Œ∑</mi><mo accent="true">‚Äæ</mo></mover></mrow><annotation encoding="application/x-tex">\frac{1}{N}\sum_p \hat{\theta}_p^{\text{CB}}
= \bar{\eta}</annotation></semantics></math> (same as PM).</li>
<li>
<strong>Match the marginal variance:</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mo stretchy="false" form="prefix">(</mo><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">CB</mtext></msubsup><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>V</mi><mi>Œ∑</mi></msub><mo>+</mo><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover></mrow><annotation encoding="application/x-tex">\text{Var}(\hat{\theta}_p^{\text{CB}})
= V_\eta + \bar{\lambda}</annotation></semantics></math> (match the
posterior expected population variance).</li>
</ol>
<p>These constraints yield the following closed-form estimator:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">CB</mtext></msubsup><mo>=</mo><mover><mi>Œ∑</mi><mo accent="true">‚Äæ</mo></mover><mo>+</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∑</mi><mi>p</mi></msub><mo>‚àí</mo><mover><mi>Œ∑</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="false" form="postfix">)</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mfrac><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><msub><mi>V</mi><mi>Œ∑</mi></msub></mfrac></mrow></msqrt><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{\theta}_p^{\text{CB}} = \bar{\eta} +
(\eta_p - \bar{\eta}) \sqrt{1 + \frac{\bar{\lambda}}{V_\eta}},
</annotation></semantics></math> where:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∑</mi><mi>p</mi></msub><mo>=</mo><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">PM</mtext></msubsup></mrow><annotation encoding="application/x-tex">\eta_p = \hat{\theta}_p^{\text{PM}}</annotation></semantics></math>
is the posterior mean for person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>,</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Œ∑</mi><mo accent="true">‚Äæ</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>‚àë</mo><mi>p</mi></msub><msub><mi>Œ∑</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\bar{\eta} = \frac{1}{N}\sum_p \eta_p</annotation></semantics></math>
is the grand mean of posterior means,</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>‚àë</mo><mi>p</mi></msub><mtext mathvariant="normal">Var</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚à£</mo><mi mathvariant="bold-italic">ùíÄ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\bar{\lambda} = \frac{1}{N}\sum_p \text{Var}(\theta_p \mid \mathbf{Y})</annotation></semantics></math>
is the mean posterior variance,</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>Œ∑</mi></msub><mo>=</mo><mtext mathvariant="normal">Var</mtext><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∑</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Œ∑</mi><mi>N</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">V_\eta = \text{Var}(\eta_1, \ldots, \eta_N)</annotation></semantics></math>
is the sample variance of the posterior means (computed with R‚Äôs
<code><a href="https://rdrr.io/r/stats/cor.html" class="external-link">var()</a></code>, i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N-1</annotation></semantics></math>
denominator).</li>
</ul>
<p>The CB estimator inflates the deviations of each posterior mean from
the grand mean by the factor
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mrow><mn>1</mn><mo>+</mo><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><mi>/</mi><msub><mi>V</mi><mi>Œ∑</mi></msub></mrow></msqrt><annotation encoding="application/x-tex">\sqrt{1 + \bar{\lambda}/V_\eta}</annotation></semantics></math>.
This factor is always greater than 1 (since
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\bar{\lambda} &gt; 0</annotation></semantics></math>),
so the CB estimates are more dispersed than the posterior means.</p>
<blockquote>
<p><strong>Interpretation of the CB scaling factor:</strong> The ratio
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><mi>/</mi><msub><mi>V</mi><mi>Œ∑</mi></msub></mrow><annotation encoding="application/x-tex">\bar{\lambda}/V_\eta</annotation></semantics></math>
measures the ‚Äúseverity of shrinkage.‚Äù When individual posterior
variances
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><annotation encoding="application/x-tex">\bar{\lambda}</annotation></semantics></math>
are large relative to the between-person variation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>Œ∑</mi></msub><annotation encoding="application/x-tex">V_\eta</annotation></semantics></math>
(i.e., the test is unreliable), the scaling factor is large and CB makes
substantial corrections. When the test is highly reliable
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Œª</mi><mo accent="true">‚Äæ</mo></mover><mo>‚â™</mo><msub><mi>V</mi><mi>Œ∑</mi></msub></mrow><annotation encoding="application/x-tex">\bar{\lambda} \ll V_\eta</annotation></semantics></math>),
CB and PM estimates are nearly identical.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="triple-goal-gr">7.4 Triple-Goal (GR)<a class="anchor" aria-label="anchor" href="#triple-goal-gr"></a>
</h3>
<p>The Triple-Goal estimator, introduced by Shen and Louis (1998),
simultaneously addresses all three goals: individual estimation,
ranking, and distribution recovery. It is the most computationally
intensive of the three methods but provides the most balanced
trade-off.</p>
<p>The GR algorithm proceeds in four steps:</p>
<p><strong>Step 1: Posterior mean ranks.</strong> For each MCMC
iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>,
rank all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
persons by their
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Œ∏</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\theta^{(s)}</annotation></semantics></math>
values. Average these ranks across iterations:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>p</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msubsup><mi>R</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mspace width="1.0em"></mspace><mrow><mtext mathvariant="normal">where </mtext><mspace width="0.333em"></mspace></mrow><msubsup><mi>R</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mtext mathvariant="normal">rank</mtext><mo stretchy="false" form="prefix">(</mo><msubsup><mi>Œ∏</mi><mi>p</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo stretchy="false" form="postfix">)</mo><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> among </mtext><mspace width="0.333em"></mspace></mrow><msubsup><mi>Œ∏</mi><mn>1</mn><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>Œ∏</mi><mi>N</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\bar{R}_p = \frac{1}{S} \sum_{s=1}^{S} R_p^{(s)}, \quad
\text{where } R_p^{(s)} = \text{rank}(\theta_p^{(s)})
\text{ among } \theta_1^{(s)}, \ldots, \theta_N^{(s)}.
</annotation></semantics></math></p>
<p>Equivalently,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\bar{R}_p</annotation></semantics></math>
can be computed as:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>p</mi></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>q</mi></msub><mo>‚â§</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚à£</mo><mi mathvariant="bold-italic">ùíÄ</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
\bar{R}_p = \sum_{q=1}^{N} P(\theta_q \leq \theta_p \mid \mathbf{Y}),
</annotation></semantics></math> which is the expected rank of person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
under the posterior.</p>
<p><strong>Step 2: Integer ranks.</strong> Convert the (generally
non-integer) posterior mean ranks to integer ranks:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo>=</mo><mtext mathvariant="normal">rank</mtext><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>p</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>N</mi><mo stretchy="false" form="postfix">}</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\hat{R}_p = \text{rank}(\bar{R}_p) \in \{1, 2, \ldots, N\}.
</annotation></semantics></math></p>
<p>Ties in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\bar{R}_p</annotation></semantics></math>
are broken randomly.</p>
<p><strong>Step 3: ISEL empirical distribution function.</strong>
Compute the <strong>integrated squared error loss</strong> (ISEL)
optimal estimator of the population EDF:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>G</mi><mo accent="true">ÃÇ</mo></mover><mi>N</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>p</mi></msub><mo>‚â§</mo><mi>t</mi><mo>‚à£</mo><mi mathvariant="bold-italic">ùíÄ</mi><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\hat{G}_N(t) = \frac{1}{N} \sum_{p=1}^{N} P(\theta_p \leq t \mid \mathbf{Y}).
</annotation></semantics></math></p>
<p>This is the posterior expectation of the true EDF and is implemented
by pooling all posterior samples across all persons.</p>
<p><strong>Step 4: Quantile inversion.</strong> Assign each person the
value from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>G</mi><mo accent="true">ÃÇ</mo></mover><mi>N</mi></msub><annotation encoding="application/x-tex">\hat{G}_N</annotation></semantics></math>
corresponding to their integer rank:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi><mtext mathvariant="normal">GR</mtext></msubsup><mo>=</mo><msubsup><mover><mi>G</mi><mo accent="true">ÃÇ</mo></mover><mi>N</mi><mrow><mi>‚àí</mi><mn>1</mn></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mn>2</mn><msub><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo>‚àí</mo><mn>1</mn></mrow><mrow><mn>2</mn><mi>N</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\hat{\theta}_p^{\text{GR}} = \hat{G}_N^{-1}\left(\frac{2\hat{R}_p - 1}{2N}\right).
</annotation></semantics></math></p>
<p>The fraction
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><msub><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><mo>‚àí</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(2\hat{R}_p - 1)/(2N)</annotation></semantics></math>
maps rank
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\hat{R}_p</annotation></semantics></math>
to the midpoint of the corresponding probability bin, preventing
boundary artifacts.</p>
<p><strong>Computational complexity.</strong> Step 1 requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>√ó</mo><mi>N</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(S \times N \log N)</annotation></semantics></math>
operations. For
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mn>5</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">S = 5{,}000</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">N = 500</annotation></semantics></math>,
this runs in seconds. In DPMirt, the GR estimator is implemented in
<code>.triple_goal()</code>, adapted from the HETOP package:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 1: Posterior mean ranks</span></span>
<span><span class="va">rbar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">s</span>, <span class="fl">1</span>, <span class="va">rank</span>, ties.method <span class="op">=</span> <span class="st">"average"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="fl">2</span>, <span class="va">mean</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: Integer ranks</span></span>
<span><span class="va">rhat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rank.html" class="external-link">rank</a></span><span class="op">(</span><span class="va">rbar</span>, ties.method <span class="op">=</span> <span class="st">"random"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Steps 3-4: ISEL EDF + quantile inversion</span></span>
<span><span class="va">theta_gr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/quantile.html" class="external-link">quantile</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">s</span><span class="op">)</span>,                            <span class="co"># Pool all posterior samples</span></span>
<span>  probs <span class="op">=</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">rhat</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">K</span><span class="op">)</span>,</span>
<span>  type <span class="op">=</span> <span class="va">quantile_type</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="comparison-of-estimators">7.5 Comparison of Estimators<a class="anchor" aria-label="anchor" href="#comparison-of-estimators"></a>
</h3>
<p>The following table summarizes the properties of the three
estimators:</p>
<table class="table">
<thead><tr class="header">
<th align="left">Property</th>
<th align="center">PM</th>
<th align="center">CB</th>
<th align="center">GR</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Optimal for individual MSE (Goal 1)</td>
<td align="center">Yes</td>
<td align="center">No</td>
<td align="center">No</td>
</tr>
<tr class="even">
<td align="left">Preserves population variance (Goal 3)</td>
<td align="center">No</td>
<td align="center">Yes</td>
<td align="center">Yes</td>
</tr>
<tr class="odd">
<td align="left">Preserves ranks (Goal 2)</td>
<td align="center">Approx.</td>
<td align="center">Approx.</td>
<td align="center">Yes</td>
</tr>
<tr class="even">
<td align="left">Closed-form from posterior samples</td>
<td align="center">Yes</td>
<td align="center">Yes</td>
<td align="center">Yes</td>
</tr>
<tr class="odd">
<td align="left">Computational cost</td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(SN)</annotation></semantics></math></td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(SN)</annotation></semantics></math></td>
<td align="center"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mi>N</mi><mrow><mi mathvariant="normal">log</mi><mo>‚Å°</mo></mrow><mi>N</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(SN \log N)</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Practical guidance:</strong> For educational testing
applications where percentile-based interpretations are important (e.g.,
reporting a student‚Äôs standing relative to the population), GR estimates
are preferred. For applications focused on individual point estimates
(e.g., adaptive testing), PM estimates are optimal. CB provides a useful
middle ground with the computational simplicity of PM.</p>
</blockquote>
</div>
</div>
<div class="section level2">
<h2 id="dp-density-reconstruction">8. DP Density Reconstruction<a class="anchor" aria-label="anchor" href="#dp-density-reconstruction"></a>
</h2>
<div class="section level3">
<h3 id="mixture-density-per-iteration">8.1 Mixture Density per Iteration<a class="anchor" aria-label="anchor" href="#mixture-density-per-iteration"></a>
</h3>
<p>At each MCMC iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>,
the DP posterior induces a mixture density:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>s</mi></msub></munderover><msub><mi>w</mi><mrow><mi>s</mi><mo>,</mo><mi>k</mi></mrow></msub><mspace width="0.167em"></mspace><mi>œï</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>;</mo><msub><mover><mi>Œº</mi><mo accent="true">ÃÉ</mo></mover><mrow><mi>s</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msubsup><mover><mi>œÉ</mi><mo accent="true">ÃÉ</mo></mover><mrow><mi>s</mi><mo>,</mo><mi>k</mi></mrow><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
f_s(x) = \sum_{k=1}^{K_s} w_{s,k} \,
\phi(x; \tilde{\mu}_{s,k}, \tilde{\sigma}^2_{s,k}),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>s</mi></msub><annotation encoding="application/x-tex">K_s</annotation></semantics></math>
is the number of occupied clusters,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mrow><mi>s</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding="application/x-tex">w_{s,k}</annotation></semantics></math>
is the cluster weight, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œï</mi><mo stretchy="false" form="prefix">(</mo><mi>‚ãÖ</mi><mo>;</mo><mi>Œº</mi><mo>,</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\phi(\cdot; \mu, \sigma^2)</annotation></semantics></math>
denotes the Normal density.</p>
</div>
<div class="section level3">
<h3 id="posterior-predictive-density-and-credible-bands">8.2 Posterior Predictive Density and Credible Bands<a class="anchor" aria-label="anchor" href="#posterior-predictive-density-and-credible-bands"></a>
</h3>
<p>The posterior predictive density averages over MCMC iterations:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{f}(x) = \frac{1}{S} \sum_{s=1}^{S} f_s(x),
</annotation></semantics></math> integrating over all sources of
uncertainty. Pointwise credible bands are obtained by taking quantiles
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup></mrow><annotation encoding="application/x-tex">\{f_s(x)\}_{s=1}^S</annotation></semantics></math>
at each grid point:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>f</mi><mo accent="true">ÃÇ</mo></mover><mtext mathvariant="normal">lower</mtext></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Q</mi><mn>0.025</mn></msub><mo stretchy="false" form="prefix">(</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">}</mo><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="1.0em"></mspace><msup><mover><mi>f</mi><mo accent="true">ÃÇ</mo></mover><mtext mathvariant="normal">upper</mtext></msup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>Q</mi><mn>0.975</mn></msub><mo stretchy="false" form="prefix">(</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">}</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\hat{f}^{\text{lower}}(x) = Q_{0.025}(\{f_s(x)\}), \quad
\hat{f}^{\text{upper}}(x) = Q_{0.975}(\{f_s(x)\}).
</annotation></semantics></math></p>
<blockquote>
<p><strong>Caveat:</strong> These are <em>pointwise</em> credible bands,
not simultaneous bands. The probability that the true density lies
within the band at every point simultaneously is less than the nominal
level.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="implementation-via-nimble">8.3 Implementation via NIMBLE<a class="anchor" aria-label="anchor" href="#implementation-via-nimble"></a>
</h3>
<p>DPMirt implements density reconstruction through the
<code><a href="../reference/dpmirt_dp_density.html">dpmirt_dp_density()</a></code> function, following Paganin et al.‚Äôs
(2023) approach: extract DP posterior samples, call NIMBLE‚Äôs
<code><a href="https://rdrr.io/pkg/nimble/man/getSamplesDPmeasure.html" class="external-link">getSamplesDPmeasure()</a></code> to obtain stick-breaking weights and
atoms, evaluate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f_s(x)</annotation></semantics></math>
on a grid, and apply rescaling.</p>
<p>For unconstrained Rasch models, the rescaling is a location shift
(Jacobian = 1):
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mi>s</mi><mtext mathvariant="normal">rescaled</mtext></msubsup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>+</mo><msup><mover><mi>Œ≤</mi><mo accent="true">‚Äæ</mo></mover><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
f_s^{\text{rescaled}}(x) = f_s(x + \bar{\beta}^{(s)}).
</annotation></semantics></math></p>
<p>For 2PL/3PL models, both location and scale adjustments are required:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mi>s</mi><mtext mathvariant="normal">rescaled</mtext></msubsup><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>‚ãÖ</mo><msub><mi>f</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mi>x</mi><mo>+</mo><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
f_s^{\text{rescaled}}(x) = d^{(s)} \cdot f_s(d^{(s)} x + c^{(s)}),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>c</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">c^{(s)}</annotation></semantics></math>
is the location shift and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>d</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">d^{(s)}</annotation></semantics></math>
is the scale factor.</p>
</div>
</div>
<div class="section level2">
<h2 id="attribution-table">9. Attribution Table<a class="anchor" aria-label="anchor" href="#attribution-table"></a>
</h2>
<p>The following table clarifies the provenance of each major component
used in the DPMirt package.</p>
<table class="table">
<colgroup>
<col width="40%">
<col width="29%">
<col width="29%">
</colgroup>
<thead><tr class="header">
<th align="left">Component</th>
<th align="left">Source</th>
<th align="left">Status</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Rasch model</td>
<td align="left">Rasch (1960); De Ayala (2022)</td>
<td align="left">Established</td>
</tr>
<tr class="even">
<td align="left">2PL / 3PL models</td>
<td align="left">Birnbaum (1968); De Ayala (2022)</td>
<td align="left">Established</td>
</tr>
<tr class="odd">
<td align="left">Dirichlet Process</td>
<td align="left">Ferguson (1973)</td>
<td align="left">Established</td>
</tr>
<tr class="even">
<td align="left">Chinese Restaurant Process</td>
<td align="left">Blackwell &amp; MacQueen (1973)</td>
<td align="left">Established</td>
</tr>
<tr class="odd">
<td align="left">Stick-breaking construction</td>
<td align="left">Sethuraman (1994)</td>
<td align="left">Established</td>
</tr>
<tr class="even">
<td align="left">DPM-IRT models + NIMBLE code</td>
<td align="left">Paganin et al.¬†(2023)</td>
<td align="left">Established (adapted)</td>
</tr>
<tr class="odd">
<td align="left">Post-hoc rescaling</td>
<td align="left">Paganin et al.¬†(2023)</td>
<td align="left">Established (adapted)</td>
</tr>
<tr class="even">
<td align="left">Posterior Mean (PM)</td>
<td align="left">Standard Bayes (Berger, 1985)</td>
<td align="left">Established</td>
</tr>
<tr class="odd">
<td align="left">Constrained Bayes (CB)</td>
<td align="left">Ghosh (1992); Louis (1984)</td>
<td align="left">Established</td>
</tr>
<tr class="even">
<td align="left">Triple-Goal (GR)</td>
<td align="left">Shen &amp; Louis (1998)</td>
<td align="left">Established</td>
</tr>
<tr class="odd">
<td align="left">Combined model-estimator framework</td>
<td align="left">Lee &amp; Wind (APM)</td>
<td align="left">Novel integration</td>
</tr>
<tr class="even">
<td align="left">Reliability-targeted simulation</td>
<td align="left">Lee (IRTsimrel)</td>
<td align="left">Novel contribution</td>
</tr>
<tr class="odd">
<td align="left">Alpha elicitation (DPprior)</td>
<td align="left">Lee (2026, DPprior)</td>
<td align="left">Novel contribution</td>
</tr>
</tbody>
</table>
</div>
<div class="section level2">
<h2 id="references">10. References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Antoniak, C. E. (1974). Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. <em>The Annals of
Statistics</em>, 2(6), 1152‚Äì1174.</p>
<p>Berger, J. O. (1985). <em>Statistical Decision Theory and Bayesian
Analysis</em> (2nd ed.). Springer.</p>
<p>Birnbaum, A. (1968). Some latent trait models and their use in
inferring an examinee‚Äôs ability. In F. M. Lord &amp; M. R. Novick
(Eds.), <em>Statistical Theories of Mental Test Scores</em>
(pp.¬†397‚Äì479). Addison-Wesley.</p>
<p>Blackwell, D., &amp; MacQueen, J. B. (1973). Ferguson distributions
via Polya urn schemes. <em>The Annals of Statistics</em>, 1(2),
353‚Äì355.</p>
<p>De Ayala, R. J. (2022). <em>The Theory and Practice of Item Response
Theory</em> (2nd ed.). Guilford Press.</p>
<p>Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric
problems. <em>The Annals of Statistics</em>, 1(2), 209‚Äì230.</p>
<p>Fox, J.-P. (2010). <em>Bayesian Item Response Modeling: Theory and
Applications</em>. Springer.</p>
<p>Ghosh, M. (1992). Constrained Bayes estimation with applications.
<em>Journal of the American Statistical Association</em>, 87(418),
533‚Äì540.</p>
<p>Ghosh, M., &amp; Kim, M.-H. (2002). The Bayes and constrained Bayes
estimators under balanced loss. <em>Statistics &amp; Probability
Letters</em>, 59(2), 175‚Äì183.</p>
<p>Lee, J. &amp; Wind, S. Targeting toward inferential goals in Bayesian
Rasch models for estimating person-specific latent traits. <em>OSF
Preprint</em>. <a href="https://doi.org/10.31219/osf.io/qrw4n" class="external-link uri">https://doi.org/10.31219/osf.io/qrw4n</a></p>
<p>Lee, J. (2025). Reliability-targeted simulation of item response
data: Solving the inverse design problem. arXiv preprint
arXiv:2512.16012. <a href="https://arxiv.org/abs/2512.16012" class="external-link uri">https://arxiv.org/abs/2512.16012</a></p>
<p>Lee, J. (2026). Design-conditional prior elicitation for Dirichlet
Process mixtures: A unified framework for cluster counts and weight
control. arXiv preprint arXiv:2602.06301. <a href="https://arxiv.org/abs/2602.06301" class="external-link uri">https://arxiv.org/abs/2602.06301</a></p>
<p>Louis, T. A. (1984). Estimating a population of parameter values
using Bayes and empirical Bayes methods. <em>Journal of the American
Statistical Association</em>, 79(386), 393‚Äì398.</p>
<p>Paganin, S., Paciorek, C. J., Wehrhahn, C., Rodr√≠guez, A.,
Rabe-Hesketh, S., &amp; de Valpine, P. (2023). Computational strategies
and estimation performance with Bayesian semiparametric item response
theory models. <em>Journal of Educational and Behavioral
Statistics</em>, 48(2), 147‚Äì188. <a href="https://doi.org/10.3102/10769986221136105" class="external-link uri">https://doi.org/10.3102/10769986221136105</a></p>
<p>Rasch, G. (1960). <em>Probabilistic Models for Some Intelligence and
Attainment Tests</em>. Danish Institute for Educational Research.</p>
<p>Sethuraman, J. (1994). A constructive definition of Dirichlet priors.
<em>Statistica Sinica</em>, 4(2), 639‚Äì650.</p>
<p>Shen, W., &amp; Louis, T. A. (1998). Triple-goal estimates in
two-stage hierarchical models. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, 60(2), 455‚Äì471.</p>
<p>Wright, B. D., &amp; Masters, G. N. (1982). <em>Rating Scale
Analysis: Rasch Measurement</em>. MESA Press.</p>
<hr>
</div>
<div class="section level2">
<h2 id="whats-next">What‚Äôs Next?<a class="anchor" aria-label="anchor" href="#whats-next"></a>
</h2>
<p>This vignette covered the mathematical foundations. For practical
guidance on using these models and methods, see:</p>
<ul>
<li><p><strong><code><a href="../articles/models-and-workflow.html">vignette("models-and-workflow", package = "DPMirt")</a></code></strong>
‚Äì Step-by-step guide to fitting Rasch, 2PL, and 3PL models with both
Normal and DPM priors. Covers the compile-once, sample-many
workflow.</p></li>
<li><p><strong><code><a href="../articles/posterior-summaries.html">vignette("posterior-summaries", package = "DPMirt")</a></code></strong>
‚Äì Practical comparison of PM, CB, and GR estimators with real data
examples. Demonstrates when each estimator is preferred.</p></li>
<li><p><strong><code><a href="../articles/nimble-internals.html">vignette("nimble-internals", package = "DPMirt")</a></code></strong>
‚Äì Under-the-hood details of NIMBLE integration, custom samplers, and the
<code><a href="https://rdrr.io/pkg/nimble/man/getSamplesDPmeasure.html" class="external-link">getSamplesDPmeasure()</a></code> workflow for DP density
reconstruction.</p></li>
</ul>
<hr>
<p><em>For questions about this vignette or the DPMirt package, please
visit the <a href="https://github.com/joonho112/DPMirt" class="external-link">GitHub
repository</a>.</em></p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://github.com/joonho112" class="external-link">JoonHo Lee</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
